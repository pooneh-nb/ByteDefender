# ByteDefender
Browser fingerprinting enables persistent cross-site user tracking via subtle techniques that often evade conventional defenses or cause website breakage when script-level blocking countermeasures are applied. Addressing these challenges requires detection methods offering both function-level precision to minimize breakage and inherent robustness against code obfuscation and URL manipulation.

We introduce ByteDefender, the first system leveraging V8 engine bytecode to detect fingerprinting operations specifically at the JavaScript function level. A Transformer-based classifier, trained offline on bytecode sequences, accurately identifies functions exhibiting fingerprinting behavior. We develop and evaluate light-weight signatures derived from this model to enable low-overhead, on-device matching against function bytecode during compilation but prior to execution, which only adds a 4% (average) latency to the page load time. This mechanism facilitates targeted, real-time prevention of fingerprinting function execution, thereby preserving legitimate script functionality. Operating directly on bytecode ensures inherent resilience against common code obfuscation and URL-based evasion. Our evaluation on the top 100k websites demonstrates high detection accuracy at both function- and script-level, with substantial improvements over state-of-the-art AST-based methods, particularly in robustness against obfuscation. ByteDefender offers a practical framework for effective, precise, and robust fingerprinting mitigation.
ðŸ“„ [Read the paper (PDF)](ByteDefender.pdf)


# ByteDefender Tool

ByteDefender is a tool designed for detecing fingerpriting practices in function level in Chromium browser.
This tool makes it possble to build a ML based model to detect fingerpriting functions, rather than scripts, using their bytecodes generated  by v8 engine in the browser.

To build a model, we need to crawl websites and collect both API traces as well as bytecodes in function level. API traces are used to build a function level ground-truth. Then this ground truth is used to label bytecodes.

# How to use
## Requirements:
    1. Install NodeJS
    2. Install Conda
    3. Clone this repository and move inside the repo directory using cd command
    3. Download the instrumented chromium from this shared drive: https://shorturl.at/bABZ9
        * Code assumes you download it in the home directory. If you want to change it, modify line 96 and 133 in crawler.py.

## Crawling
    1. Open three terminals inside repository directory 
    
    ---- First Terminal ----
    2. In first terminal, run cd server and then node server.js -- this will start the local-host server at Port:4000 which communicates with chrome-extension to save the captured API traces inside server/output directory. 
    ---- First Terminal ----
    3. In second terminal, run cd server and then node server2.js -- this will start the local-host server at Port:5000 which communicates with chrome-extension to save the captured script sources inside server/output directory. 
    ---- Third Terminal ----
    4. In second terminal, create conda environment by running `$ conda env create -f environment.yml`
    5. run the crawler.py file. It will start crawling top 1000 websites from tranco list. 

## Output
The output of crawling is kept in /server/output. For each websites 2 files and 1 directory are generated:

    1. apiTraces.json
    2. scripts.json
    3. bytecodes
        3.1 include all bytecodes generated by each thread

## Steps required to process raw data generated in crawling and build the ML models
After crawling through all websites, several steps are required to transition from raw data to machine learning evaluation results.

### Post processing raw data
Execute three methods from the `post_processing_crawled_sites` module to filter out any incomplete or erroneous sites from the crawled data:
    
    1. remove_incomplete_crawled_sites.py
    2. remove_errori_crawled_sites.py
    3. remove_repeated_crawled sites.py

### Create DataBase
Run create_DB.py from `create_DataBase` module:  
We go through each website and create a database which technically says for each website:

    1. what scripts are available (no matter if it has URL or not, we detect them using script_Id) 
    2. for each script what functions are available (we only work on non-anonymized functions; i.e. functions with names)
    3. For each function
        3.1 What is the bytecode 
        3.2 What are the API traces
        3.3 If the function is fingerprinting or not (based on API traces and our heuristics)

### Build a ML_model (Option1)

A. Run data_processing.py from `ML_Model` module:  
    This process creates a flat database. It includes 5 columns:

    * Site name
    * script_id
    * Function name
    * Tokenized bytecodes(separated with comma); List of words
    * True or False labels (True for FP and False for non-FP)

B. Run X_y.py from `ML_Model` module:

This function will generate a two column dataset of bytecodes (list of words) and binary labels. 


C. Run word_level_embeddings.py from `ML_Model` module:  
Train an embedding model for our dataset using 2 techniques: word2vec, FastText
If you want to have document level embedings, run document_level_embedding.py from `ML_Model`

D. Run random_forest.py classifier from `ML_Model` module:  
Run the random forest classifier on both embeddings.

Understating the input or X:  

    X is an array of vectorized text data. For each bytecode in bytecode_sequences, vectorize_text is called to convert it into a vector, and the resulting vectors are formed into a NumPy array.
    The retrieved word vectors are then averaged using np.mean(vectors, axis=0) to create a single vector representation for the entire sequence of tokens. 

Understanding the label type or y:

    y is an array of 0/1 labels

### Build a ML_model (Option2)

Our results in the paper are trained and tested based on a model in `~/ML_Model/transformer/bytecodes_for_paper`
